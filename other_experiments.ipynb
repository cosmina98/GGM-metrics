{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from evaluation.mol_structure import list_of_smiles_to_nx_graphs\n",
    "from evaluation.mol_structure import draw_one_mol, draw_graphs\n",
    "from evaluation.new_structural_metric import symmetric_graph_set_distance,atom,cycle,neighborhood\n",
    "from evaluate import evaluate\n",
    "#import utils.graph_generators as gen\n",
    "import torch\n",
    "import networkx as nx\n",
    "import os \n",
    "import sys\n",
    "import json\n",
    "current = os.getcwd()\n",
    "parent = os.path.dirname(current)\n",
    "sys.path.append(parent)\n",
    "import rdkit\n",
    "import numpy as np\n",
    "from evaluation.utils import get_data,get_graph_data,  get_mock_data,remove_empty_graphs_and_targets,\\\n",
    "    get_generated_data,preprocess,get_generated_graph_data,_preprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_rewired_graph_data(name,percent):\n",
    "    generated_neg_graphs=pd.read_pickle(f'data/rewired/{name}_train2_neg/{name}_train2_neg_rewired_with_{percent}.pkl')\n",
    "    generated_pos_graphs=pd.read_pickle(f'data/rewired/{name}_train2_pos/{name}_train2_pos_rewired_with_{percent}.pkl')\n",
    "    generated_graphs,generated_targets = generated_pos_graphs + generated_neg_graphs, [1]*len(generated_pos_graphs)+[0]*len(generated_neg_graphs)\n",
    "    return  generated_graphs, np.array(generated_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic_1_50\n",
      "Now computing classifier based metrics\n",
      "WARNING:tensorflow:From C:\\Users\\cosmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow_gan\\python\\eval\\classifier_metrics.py:1157: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "Now computing structural based metrics\n",
      "Cannot compute these structural metrics\n",
      "Now computing the auc_roc based  metric\n"
     ]
    }
   ],
   "source": [
    "structural_statistic=['WL', 'nspdk','degree','cluster', 'spectral']\n",
    "device=torch.device('cpu')\n",
    "metrics_type=['nn','structural', 'auc_roc']\n",
    "generators=['swingnn','gdss']\n",
    "splits=['_50']\n",
    "rewiring_ratio=['5','10','25','33','40','50','66','75','90','100']\n",
    "metrics={}\n",
    "for generator in ['rewired']:\n",
    "    for dataset_name in ['synthetic_1','synthetic_2']:\n",
    "        for split in splits:\n",
    "            for percent in rewiring_ratio:\n",
    "                print(dataset_name+split)\n",
    "                train1_graphs,train1_targets,train2_graphs,train2_targets,test_graphs,test_targets=get_graph_data(dataset_name+split, path=r'data/graphs/datasets')\n",
    "                generated_graphs, generated_targets= get_rewired_graph_data(dataset_name+split,percent)\n",
    "                \n",
    "                splits_for_auc_roc=[train1_graphs , train1_targets,train2_graphs , train2_targets, test_graphs, test_targets, generated_graphs, generated_targets]\n",
    "                metrics=evaluate(train1_graphs, generated_graphs, device,  metrics_type, structural_statistic,*splits_for_auc_roc)\n",
    "                metrics['dataset']=dataset_name+split\n",
    "                metrics['generator_name']=generator\n",
    "                metrics['degree_of_perturbation']=percent\n",
    "                print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pertubed with ER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_corrupted_graph_data(name,percent):\n",
    "    generated_neg_graphs=pd.read_pickle(f'data/corrupted/{name}_train2_neg/{name}_train2_neg_perturbed_with_{percent}.pkl')\n",
    "    generated_pos_graphs=pd.read_pickle(f'data/corrupted/{name}_train2_pos/{name}_train2_pos_perturbed_with_{percent}.pkl')\n",
    "    generated_graphs,generated_targets = generated_pos_graphs + generated_neg_graphs, [1]*len(generated_pos_graphs)+[0]*len(generated_neg_graphs)\n",
    "    return  generated_graphs, np.array(generated_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic_1_50\n",
      "Now computing classifier based metrics\n",
      "Cannot compute the classifier based metrics\n",
      "Now computing structural based metrics\n",
      "Cannot compute these structural metrics\n",
      "Now computing the auc_roc based  metric\n"
     ]
    }
   ],
   "source": [
    "structural_statistic=['WL', 'nspdk','degree','cluster', 'spectral']\n",
    "device=torch.device('cpu')\n",
    "metrics_type=['nn', 'structural', 'auc_roc']\n",
    "generators=['swingnn','gdss']\n",
    "splits=['_50']\n",
    "rewiring_ratio=['5','10','25','33','40','50','66','75','90','100']\n",
    "metrics={}\n",
    "for generator in ['corrupted']:\n",
    "    for dataset_name in ['synthetic_1','synthetic_1']:\n",
    "        for split in splits:\n",
    "            for percent in rewiring_ratio:\n",
    "                print(dataset_name+split)\n",
    "                train1_graphs,train1_targets,train2_graphs,train2_targets,test_graphs,test_targets=get_graph_data(dataset_name+split, path=r'data/graphs/datasets')\n",
    "                generated_graphs, generated_targets= get_corrupted_graph_data(dataset_name+split,percent)\n",
    "                splits_for_auc_roc=[train1_graphs , train1_targets,train2_graphs , train2_targets, test_graphs, test_targets, generated_graphs, generated_targets]\n",
    "                metrics=evaluate(train1_graphs, generated_graphs, device,  metrics_type, structural_statistic,*splits_for_auc_roc)\n",
    "                metrics['dataset']=dataset_name+split\n",
    "                metrics['generator_name']=generator\n",
    "                metrics['degree_of_perturbation']=percent\n",
    "                print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mode Dropping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_mode_dropping_graph_data(name,percent):\n",
    "    #try:\n",
    "   generated_graphs=pd.read_pickle(f'data/mode_dropping/{name}/{name}_no_of_clusters_dropped_{percent}.pkl')\n",
    "   targets=[]\n",
    "   for i, g in enumerate(generated_graphs):\n",
    "        target= int(g.nodes[0]['target'])\n",
    "        targets.append(target)\n",
    "   return  generated_graphs, np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic_1_50\n",
      "Now computing classifier based metrics\n",
      "Cannot compute the classifier based metrics\n",
      "Now computing structural based metrics\n"
     ]
    }
   ],
   "source": [
    "structural_statistic=['WL', 'nspdk','degree','cluster', 'spectral']\n",
    "device=torch.device('cpu')\n",
    "metrics_type=['nn', 'structural', 'auc_roc']\n",
    "generators=['swingnn','gdss']\n",
    "splits=['_50']\n",
    "rewiring_ratio=[i for i in range(1, 32)]\n",
    "metrics={}\n",
    "for generator in ['mode_dropping']:\n",
    "    for dataset_name in ['synthetic_1']:\n",
    "        for split in splits:\n",
    "            for percent in rewiring_ratio:\n",
    "                print(dataset_name+split)\n",
    "                train1_graphs,train1_targets,train2_graphs,train2_targets,test_graphs,test_targets=get_graph_data(dataset_name+split, path=r'data/graphs/datasets')\n",
    "                generated_graphs, generated_targets= get_mode_dropping_graph_data(dataset_name+split,percent)\n",
    "                splits_for_auc_roc=[train1_graphs , train1_targets,train2_graphs , train2_targets, test_graphs, test_targets, generated_graphs, generated_targets]\n",
    "                metrics=evaluate(train1_graphs, generated_graphs, device,  metrics_type, structural_statistic,*splits_for_auc_roc)\n",
    "                metrics['dataset']=dataset_name+split\n",
    "                metrics['generator_name']=generator\n",
    "                metrics['degree_of_perturbation']=percent\n",
    "                print(metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structural_statistic=['WL', 'nspdk','degree','cluster', 'spectral']\n",
    "device=torch.device('cpu')\n",
    "metrics_type=['nn', 'structural', 'auc_roc']\n",
    "generators=['swingnn','gdss']\n",
    "splits=['_50']\n",
    "rewiring_ratio=[i for i in range(1, 33)]\n",
    "metrics={}\n",
    "for generator in ['mode_dropping']:\n",
    "    for dataset_name in ['synthetic_2']:\n",
    "        for split in splits:\n",
    "            for percent in rewiring_ratio:\n",
    "                print(dataset_name+split)\n",
    "                train1_graphs,train1_targets,train2_graphs,train2_targets,test_graphs,test_targets=get_graph_data(dataset_name+split, path=r'data/graphs/datasets')\n",
    "                generated_graphs, generated_targets= get_mode_dropping_graph_data(dataset_name+split,percent)\n",
    "                splits_for_auc_roc=[train1_graphs , train1_targets,train2_graphs , train2_targets, test_graphs, test_targets, generated_graphs, generated_targets]\n",
    "                metrics=evaluate(train1_graphs, generated_graphs, device,  metrics_type, structural_statistic,*splits_for_auc_roc)\n",
    "                metrics['dataset']=dataset_name+split\n",
    "                metrics['generator_name']=generator\n",
    "                metrics['degree_of_perturbation']=percent\n",
    "                print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mode Collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "def get_mode_collapse_graph_data(name,percent):\n",
    "    #try:\n",
    "   generated_graphs=pd.read_pickle(f'data/mode_collapse/{name}/{name}_no_of_clusters_collapsed_{percent}.pkl')\n",
    "   targets=[]\n",
    "   for i, g in enumerate(generated_graphs):\n",
    "        target= int(g.nodes[0]['target'])\n",
    "        targets.append(target)\n",
    "   return  generated_graphs, np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic_1_50\n",
      "Now computing classifier based metrics\n",
      "Cannot compute the classifier based metrics\n",
      "Now computing structural based metrics\n",
      "Cannot compute these structural metrics\n",
      "Now computing the auc_roc based  metric\n"
     ]
    }
   ],
   "source": [
    "structural_statistic=['WL', 'nspdk','degree','cluster', 'spectral']\n",
    "device=torch.device('cpu')\n",
    "metrics_type=['nn', 'structural', 'auc_roc']\n",
    "generators=['swingnn','gdss']\n",
    "splits=['_50']\n",
    "rewiring_ratio=[i for i in range(1, 32)]\n",
    "metrics={}\n",
    "for generator in ['mode_collapse']:\n",
    "    for dataset_name in ['synthetic_1']:\n",
    "        for split in splits:\n",
    "            for percent in rewiring_ratio:\n",
    "                print(dataset_name+split)\n",
    "                train1_graphs,train1_targets,train2_graphs,train2_targets,test_graphs,test_targets=get_graph_data(dataset_name+split, path=r'data/graphs/datasets')\n",
    "                generated_graphs, generated_targets= get_mode_collapse_graph_data(dataset_name+split,percent)\n",
    "                splits_for_auc_roc=[train1_graphs , train1_targets,train2_graphs , train2_targets, test_graphs, test_targets, generated_graphs, generated_targets]\n",
    "                metrics=evaluate(train1_graphs, generated_graphs, device,  metrics_type, structural_statistic,*splits_for_auc_roc)\n",
    "                metrics['dataset']=dataset_name+split\n",
    "                metrics['generator_name']=generator\n",
    "                metrics['degree_of_perturbation']=percent\n",
    "                print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structural_statistic=['WL', 'nspdk','degree','cluster', 'spectral']\n",
    "device=torch.device('cpu')\n",
    "metrics_type=['nn', 'structural', 'auc_roc']\n",
    "generators=['swingnn','gdss']\n",
    "splits=['_50']\n",
    "rewiring_ratio=[i for i in range(1, 33)]\n",
    "metrics={}\n",
    "for generator in ['mode_collapse']:\n",
    "    for dataset_name in ['synthetic_2']:\n",
    "        for split in splits:\n",
    "            for percent in rewiring_ratio:\n",
    "                print(dataset_name+split)\n",
    "                train1_graphs,train1_targets,train2_graphs,train2_targets,test_graphs,test_targets=get_graph_data(dataset_name+split, path=r'data/graphs/datasets')\n",
    "                generated_graphs, generated_targets= get_mode_collapse_graph_data(dataset_name+split,percent)\n",
    "                splits_for_auc_roc=[train1_graphs , train1_targets,train2_graphs , train2_targets, test_graphs, test_targets, generated_graphs, generated_targets]\n",
    "                metrics=evaluate(train1_graphs, generated_graphs, device,  metrics_type, structural_statistic,*splits_for_auc_roc)\n",
    "                metrics['dataset']=dataset_name+split\n",
    "                metrics['generator_name']=generator\n",
    "                metrics['degree_of_perturbation']=percent\n",
    "                print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additionally train1 versus train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic_2_50\n",
      "Now computing classifier based metrics\n",
      "Cannot compute the classifier based metrics\n",
      "Now computing structural based metrics\n",
      "Cannot compute these structural metrics\n",
      "Now computing the auc_roc based  metric\n"
     ]
    }
   ],
   "source": [
    "structural_statistic=['WL', 'nspdk','degree','cluster', 'spectral']\n",
    "device=torch.device('cpu')\n",
    "metrics_type=['nn', 'structural', 'auc_roc']\n",
    "generators=['swingnn','gdss']\n",
    "splits=['_50']\n",
    "rewiring_ratio=[i for i in range(1, 33)]\n",
    "metrics={}\n",
    "for generator in ['train2']:\n",
    "    for dataset_name in ['synthetic_1','synthetic_2']:\n",
    "        for split in splits:\n",
    "            print(dataset_name+split)\n",
    "            train1_graphs,train1_targets,train2_graphs,train2_targets,test_graphs,test_targets=get_graph_data(dataset_name+split, path=r'data/graphs/datasets')\n",
    "            generated_graphs, generated_targets= train2_graphs, train2_targets\n",
    "            splits_for_auc_roc=[train1_graphs , train1_targets,train2_graphs , train2_targets, test_graphs, test_targets, generated_graphs, generated_targets]\n",
    "            metrics=evaluate(train1_graphs, generated_graphs, device,  metrics_type, structural_statistic,*splits_for_auc_roc)\n",
    "            metrics['dataset']=dataset_name+split\n",
    "            metrics['generator_name']=generator\n",
    "            metrics['degree_of_perturbation']=percent\n",
    "            print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWINGNN as we increase the number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from sklearn.utils import shuffle                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
    "from evaluation.mol_structure import list_of_smiles_to_nx_graphs\n",
    "from evaluation.utils import remove_empty_graphs_and_targets\n",
    "import numpy as np\n",
    "def get_swingnn_graph_data(name,percent,split,path='data/SWINGNN_experiments/'):\n",
    "     path+=f'{percent}-percent/'\n",
    "     path_postives=path+r'{}/{}/pos/*.pkl'.format(name,name+split)\n",
    "     files = glob(path_postives)\n",
    "     generated_pos_graphs=pd.read_pickle(files[0])\n",
    "     path_negatives=path+r'{}/{}/neg/*.pkl'.format(name,name+split)\n",
    "     files = glob(path_negatives)\n",
    "     generated_neg_graphs=pd.read_pickle(files[0])\n",
    "     generated_graphs,generated_targets = generated_pos_graphs + generated_neg_graphs, [1]*len(generated_pos_graphs)+[0]*len(generated_neg_graphs)\n",
    "\n",
    "     return  generated_graphs, np.array(generated_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='synthetic_1'\n",
    "percent='100'\n",
    "split='_50'\n",
    "get_swingnn_graph_data(name,percent,split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.utils import shuffle                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
    "from evaluation.mol_structure import list_of_smiles_to_nx_graphs\n",
    "from evaluation.utils import remove_empty_graphs_and_targets\n",
    "import numpy as np\n",
    "\n",
    "def get_swingnn_molecular_data(name,percent, path='data/SWINGNN_experiments/', generator_name='stgg',return_smiles=False):\n",
    "     path+=f'{percent}-percent/'\n",
    "     pos_list, neg_list=[],[]\n",
    "     path_postives=path+r'{}/{}/pos/*.txt'.format(name,name+'_50')\n",
    "     files = glob(path_postives)\n",
    "     \n",
    "     with open(files[0]) as my_file:\n",
    "          for line in my_file:\n",
    "               pos_list.append(line.strip())\n",
    "\n",
    "     path_negatives=path+r'{}/{}/neg/*.txt'.format(name,name+'_50')\n",
    "     files = glob(path_negatives)\n",
    "     \n",
    "     with open(files[0]) as my_file:\n",
    "          for line in my_file:\n",
    "               neg_list.append(line.strip())\n",
    "\n",
    "     pos_graphs=list_of_smiles_to_nx_graphs(pos_list)\n",
    "     neg_graphs=list_of_smiles_to_nx_graphs(neg_list)\n",
    "     #print(len(neg_graphs))\n",
    "     generated_graphs,generated_targets = pos_graphs + neg_graphs, [1]*len(pos_graphs)+[0]*len(neg_graphs)\n",
    "     #generated_graphs, generated_targets = shuffle(generated_graphs, generated_targets)\n",
    "     generated_graphs, generated_targets = remove_empty_graphs_and_targets(generated_graphs, generated_targets)\n",
    "     if return_smiles:\n",
    "          return  generated_graphs, generated_targets,pos_list+neg_list\n",
    "     return  generated_graphs, np.array(generated_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='ames'\n",
    "percent='100'\n",
    "split='_50'\n",
    "_,generated_targets, generated_smiles=get_swingnn_molecular_data(name,percent,return_smiles=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_targets[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Molecular graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn and structural "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ames_50\n",
      "['data/SWINGNN_experiments/5-percent/ames/ames_50/pos\\\\gen_smiles.txt']\n",
      "Now computing classifier based metrics\n",
      "WARNING:tensorflow:From C:\\Users\\cosmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow_gan\\python\\eval\\classifier_metrics.py:1157: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "Now computing structural based metrics\n",
      "Cannot compute these structural metrics\n",
      "{'activations_time': 0.9547960758209229, 'fid': 33.27688554415545, 'fid_time': 0.966336727142334, 'kid': 0.92367905, 'kid_time': 47.91331601142883, 'precision': 0.900667451904201, 'recall': 0.6486062033765214, 'f1_pr': 0.7541423633017976, 'precision_time': 1.9263620376586914, 'recall_time': 1.9263620376586914, 'f1_pr_time': 1.9263620376586914, 'density': 0.7696113074204947, 'coverage': 0.3541421279937181, 'f1_dc': 0.4850853653998528, 'density_time': 1.5020718574523926, 'coverage_time': 1.5020718574523926, 'f1_dc_time': 1.5020718574523926, 'mmd_rbf': 0.1818777322769165, 'mmd_rbf_time': 3.0525259971618652, 'mmd_linear': 19.38178, 'mmd_linear_time': 0.9557986259460449, 'dataset': 'ames_50', 'generator_name': 'swingnn'}\n",
      "ames_50\n",
      "['data/SWINGNN_experiments/10-percent/ames/ames_50/pos\\\\gen_smiles (1).txt']\n",
      "Now computing classifier based metrics\n",
      "Now computing structural based metrics\n"
     ]
    }
   ],
   "source": [
    "current_smiles_datasets=['ames', 'bbb_martins', 'cyp1a2_veith', 'cyp2c19_veith','herg_karim','lipophilicity_astrazeneca']\n",
    "splits=['_25','_33','_40','_50']\n",
    "device=torch.device('cpu')\n",
    "structural_statistic=['WL', 'nspdk','degree','cluster', 'spectral']#'common_substructures']\n",
    "metrics_type=['nn','structural']#'molecular']\n",
    "#leave it blank to compute all the structural structures \n",
    "generators=['swingnn']\n",
    "experiments=[5,10,25,33,40,50,66,75,100]\n",
    "device=torch.device('cpu')\n",
    "metrics={}\n",
    "split='_50'\n",
    "for generator in generators:\n",
    "    for dataset_name in current_smiles_datasets:\n",
    "             for percent in experiments:\n",
    "                print(dataset_name+split)\n",
    "                train1_graphs , train1_targets,train2_graphs , train2_targets, test_graphs, test_targets  =get_data(dataset_name+split, return_smiles=False)\n",
    "                generated_graphs, generated_targets=get_swingnn_molecular_data(dataset_name,percent)\n",
    "                splits_for_auc_roc=[train1_graphs , train1_targets,train2_graphs , train2_targets, test_graphs, test_targets, generated_graphs, generated_targets]\n",
    "                metrics=evaluate(train1_graphs, generated_graphs, device,  metrics_type, structural_statistic,*splits_for_auc_roc)\n",
    "                metrics['dataset']=dataset_name+split\n",
    "                metrics['generator_name']=generator\n",
    "                metrics['numer_of_epochs_trained']=(percent*5000)/100\n",
    "                print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ames_25\n",
      "Now computing classifier based metrics\n",
      "WARNING:tensorflow:From C:\\Users\\cosmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow_gan\\python\\eval\\classifier_metrics.py:1157: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "Now computing structural based metrics\n",
      "Cannot compute these structural metrics\n",
      "{'activations_time': 0.41626691818237305, 'fid': 13.457352981320554, 'fid_time': 0.42125964164733887, 'kid': -1.0777628, 'kid_time': 6.520142555236816, 'precision': 0.9646504320502749, 'recall': 0.9795758051846033, 'f1_pr': 0.97206582989574, 'precision_time': 0.7290303707122803, 'recall_time': 0.7290303707122803, 'f1_pr_time': 0.7290303707122803, 'density': 0.9558523173605655, 'coverage': 0.9701492537313433, 'f1_dc': 0.962957722176193, 'density_time': 0.6316354274749756, 'coverage_time': 0.6316354274749756, 'f1_dc_time': 0.6316354274749756, 'mmd_rbf': 0.002143561840057373, 'mmd_rbf_time': 1.0874826908111572, 'mmd_linear': 0.1571195, 'mmd_linear_time': 0.41765594482421875, 'wl_mmd': 0.0015710919088766694, 'wl_mmd_time': 3.7544023990631104, 'dataset': 'ames_25', 'generator_name': 'swingnn'}\n",
      "ames_33\n"
     ]
    }
   ],
   "source": [
    "current_smiles_datasets=['ames', 'bbb_martins', 'cyp1a2_veith', 'cyp2c19_veith','herg_karim','lipophilicity_astrazeneca']\n",
    "device=torch.device('cpu')\n",
    "structural_statistic=['WL', 'nspdk','degree','cluster', 'spectral']#'common_substructures']\n",
    "metrics_type=['nn','structural']#'molecular']\n",
    "#leave it blank to compute all the structural structures \n",
    "generators=['train2']\n",
    "experiments=[5,10,25,33,40,50,66,75,100]\n",
    "device=torch.device('cpu')\n",
    "metrics={}\n",
    "split='_50'\n",
    "for generator in generators:\n",
    "    for dataset_name in current_smiles_datasets:\n",
    "        for split in splits:\n",
    "            print(dataset_name+split)\n",
    "            train1_graphs , train1_targets,train2_graphs , train2_targets, test_graphs, test_targets  =get_data(dataset_name+split, return_smiles=False)\n",
    "            generated_graphs, generated_targets=train2_graphs , train2_targets\n",
    "            splits_for_auc_roc=[train1_graphs , train1_targets,train2_graphs , train2_targets, test_graphs, test_targets, generated_graphs, generated_targets]\n",
    "            metrics=evaluate(train1_graphs, generated_graphs, device,  metrics_type, structural_statistic,*splits_for_auc_roc)\n",
    "            metrics['dataset']=dataset_name+split\n",
    "            metrics['generator_name']=generator\n",
    "            print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### molecular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.moses.metrics.metrics import get_all_metrics\n",
    "current_smiles_datasets=['ames', 'bbb_martins', 'cyp1a2_veith', 'cyp2c19_veith','herg_karim','lipophilicity_astrazeneca']\n",
    "split['_50'\n",
    "device=torch.device('cpu')\n",
    "structural_statistic=['WL', 'nspdk','degree','cluster', 'spectral']#'common_substructures']\n",
    "metrics_type=['nn','structural']#'molecular']\n",
    "#leave it blank to compute all the structural structures \n",
    "generators=['swingnn']\n",
    "experiments=[5,10,25,33,40,50,66,75,100]\n",
    "device=torch.device('cpu')\n",
    "metrics={}\n",
    "for generator in generators:\n",
    "    for dataset_name in current_smiles_datasets:\n",
    "            for percent in experiments:\n",
    "                print(dataset_name+split)\n",
    "                graphs, splits =get_data(dataset_name+split, return_smiles=True)\n",
    "                train1_smiles=splits['train1_pos_smiles']+splits['train1_neg_smiles'] \n",
    "                _,generated_targets, generated_smiles=get_swingnn_molecular_data(name,percent,return_smiles=True) \n",
    "                metrics=get_all_metrics(gen=generated_smiles,train=train1_smiles,test=train1_smiles)\n",
    "                metrics['dataset']=dataset_name+split\n",
    "                metrics['generator_name']=generator\n",
    "                metrics['numer_of_epochs_trained']=(percent*5000)/100 \n",
    "                print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ames_50\n",
      "{'valid': 1.0, 'unique@1000': 0.999, 'unique@10000': 0.9988221436984688, 'FCD/Test': 0.8156919603441324, 'SNN/Test': 0.5756633373852753, 'Frag/Test': 0.9878820451873492, 'Scaf/Test': 0.8593618010840082, 'FCD/TestSF': 22.184285088577205, 'SNN/TestSF': 0.33677946461981123, 'Frag/TestSF': -0.02581333437055111, 'Scaf/TestSF': 0.20333785426573803, 'IntDiv': 0.9102053951183938, 'IntDiv2': 0.8921645781772736, 'Filters': 0.5618374558303887, 'logP': 0.08376508833922258, 'SA': 0.04752492131394497, 'QED': 0.0039007607322941876, 'weight': 4.032424028268554, 'Novelty': 0.9976415094339622, 'dataset': 'ames_50', 'generator_name': 'swingnn', 'numer_of_epochs_trained': 250.0}\n",
      "bbb_martins_50\n",
      "{'valid': 1.0, 'unique@1000': 0.9957805907172996, 'unique@10000': 0.9957805907172996, 'FCD/Test': 3.6841985920182054, 'SNN/Test': 0.5064036945397173, 'Frag/Test': 0.966294915338154, 'Scaf/Test': 0.5502519513284054, 'FCD/TestSF': 20.86298113850406, 'SNN/TestSF': 0.3738190557404242, 'Frag/TestSF': 0.06172323226684595, 'Scaf/TestSF': 0.02450489523326016, 'IntDiv': 0.8879810418473504, 'IntDiv2': 0.8712773076208551, 'Filters': 0.7848101265822784, 'logP': 0.1994175822190528, 'SA': 0.09684933449687291, 'QED': 0.018486706726551186, 'weight': 18.426579152552424, 'Novelty': 0.980225988700565, 'dataset': 'bbb_martins_50', 'generator_name': 'swingnn', 'numer_of_epochs_trained': 250.0}\n",
      "cyp1a2_veith_50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cosmi\\Documents\\PhD\\2023\\Proposal\\Graph_Evaluation_Benchmark\\other_experiments.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cosmi/Documents/PhD/2023/Proposal/Graph_Evaluation_Benchmark/other_experiments.ipynb#X40sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m train1_smiles\u001b[39m=\u001b[39msplits[\u001b[39m'\u001b[39m\u001b[39mtrain1_pos_smiles\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m+\u001b[39msplits[\u001b[39m'\u001b[39m\u001b[39mtrain1_neg_smiles\u001b[39m\u001b[39m'\u001b[39m] \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cosmi/Documents/PhD/2023/Proposal/Graph_Evaluation_Benchmark/other_experiments.ipynb#X40sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m train2_smiles\u001b[39m=\u001b[39msplits[\u001b[39m'\u001b[39m\u001b[39mtrain2_pos_smiles\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m+\u001b[39msplits[\u001b[39m'\u001b[39m\u001b[39mtrain2_neg_smiles\u001b[39m\u001b[39m'\u001b[39m] \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/cosmi/Documents/PhD/2023/Proposal/Graph_Evaluation_Benchmark/other_experiments.ipynb#X40sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m metrics\u001b[39m=\u001b[39mget_all_metrics(gen\u001b[39m=\u001b[39;49mtrain2_smiles,train\u001b[39m=\u001b[39;49mtrain1_smiles,test\u001b[39m=\u001b[39;49mtrain1_smiles)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cosmi/Documents/PhD/2023/Proposal/Graph_Evaluation_Benchmark/other_experiments.ipynb#X40sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m metrics[\u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mdataset_name\u001b[39m+\u001b[39msplit\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cosmi/Documents/PhD/2023/Proposal/Graph_Evaluation_Benchmark/other_experiments.ipynb#X40sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m metrics[\u001b[39m'\u001b[39m\u001b[39mgenerator_name\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mgenerator\n",
      "File \u001b[1;32mc:\\Users\\cosmi\\Documents\\PhD\\2023\\Proposal\\Graph_Evaluation_Benchmark\\evaluation\\moses\\metrics\\metrics.py:102\u001b[0m, in \u001b[0;36mget_all_metrics\u001b[1;34m(gen, k, n_jobs, device, batch_size, pool, test, test_scaffolds, ptest, ptest_scaffolds, train)\u001b[0m\n\u001b[0;32m     99\u001b[0m     metrics[\u001b[39m'\u001b[39m\u001b[39munique@\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(_k)] \u001b[39m=\u001b[39m fraction_unique(gen, _k, pool)\n\u001b[0;32m    101\u001b[0m \u001b[39mif\u001b[39;00m ptest \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     ptest \u001b[39m=\u001b[39m compute_intermediate_statistics(test, n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    103\u001b[0m                                             device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    104\u001b[0m                                             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    105\u001b[0m                                             pool\u001b[39m=\u001b[39;49mpool)\n\u001b[0;32m    106\u001b[0m \u001b[39mif\u001b[39;00m test_scaffolds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m ptest_scaffolds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     ptest_scaffolds \u001b[39m=\u001b[39m compute_intermediate_statistics(\n\u001b[0;32m    108\u001b[0m         test_scaffolds, n_jobs\u001b[39m=\u001b[39mn_jobs,\n\u001b[0;32m    109\u001b[0m         device\u001b[39m=\u001b[39mdevice, batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m    110\u001b[0m         pool\u001b[39m=\u001b[39mpool\n\u001b[0;32m    111\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\cosmi\\Documents\\PhD\\2023\\Proposal\\Graph_Evaluation_Benchmark\\evaluation\\moses\\metrics\\metrics.py:179\u001b[0m, in \u001b[0;36mcompute_intermediate_statistics\u001b[1;34m(smiles, n_jobs, device, batch_size, pool)\u001b[0m\n\u001b[0;32m    175\u001b[0m statistics[\u001b[39m'\u001b[39m\u001b[39mScaf\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m ScafMetric(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39mprecalc(mols)\n\u001b[0;32m    176\u001b[0m \u001b[39mfor\u001b[39;00m name, func \u001b[39min\u001b[39;00m [(\u001b[39m'\u001b[39m\u001b[39mlogP\u001b[39m\u001b[39m'\u001b[39m, logP), (\u001b[39m'\u001b[39m\u001b[39mSA\u001b[39m\u001b[39m'\u001b[39m, SA),\n\u001b[0;32m    177\u001b[0m                    (\u001b[39m'\u001b[39m\u001b[39mQED\u001b[39m\u001b[39m'\u001b[39m, QED),\n\u001b[0;32m    178\u001b[0m                    (\u001b[39m'\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m'\u001b[39m, weight)]:\n\u001b[1;32m--> 179\u001b[0m     statistics[name] \u001b[39m=\u001b[39m WassersteinMetric(func, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mprecalc(mols)\n\u001b[0;32m    180\u001b[0m \u001b[39mif\u001b[39;00m close_pool:\n\u001b[0;32m    181\u001b[0m     pool\u001b[39m.\u001b[39mterminate()\n",
      "File \u001b[1;32mc:\\Users\\cosmi\\Documents\\PhD\\2023\\Proposal\\Graph_Evaluation_Benchmark\\evaluation\\moses\\metrics\\metrics.py:341\u001b[0m, in \u001b[0;36mWassersteinMetric.precalc\u001b[1;34m(self, mols)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprecalc\u001b[39m(\u001b[39mself\u001b[39m, mols):\n\u001b[0;32m    340\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 341\u001b[0m         values \u001b[39m=\u001b[39m mapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, mols)\n\u001b[0;32m    342\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m         values \u001b[39m=\u001b[39m mols\n",
      "File \u001b[1;32mc:\\Users\\cosmi\\Documents\\PhD\\2023\\Proposal\\Graph_Evaluation_Benchmark\\evaluation\\moses\\utils.py:115\u001b[0m, in \u001b[0;36mmapper.<locals>._mapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\cosmi\\Documents\\PhD\\2023\\Proposal\\Graph_Evaluation_Benchmark\\evaluation\\moses\\metrics\\utils.py:59\u001b[0m, in \u001b[0;36mQED\u001b[1;34m(mol)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mQED\u001b[39m(mol):\n\u001b[0;32m     56\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m    Computes RDKit's QED score\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m qed(mol)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\rdkit\\Chem\\QED.py:203\u001b[0m, in \u001b[0;36mqed\u001b[1;34m(mol, w, qedProperties)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Calculate the weighted sum of ADS mapped properties\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \n\u001b[0;32m    191\u001b[0m \u001b[39msome examples from the QED paper, reference values from Peter G's original implementation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39m0.234...\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39mif\u001b[39;00m qedProperties \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m   qedProperties \u001b[39m=\u001b[39m properties(mol)\n\u001b[0;32m    204\u001b[0m d \u001b[39m=\u001b[39m [ads(pi, adsParameters[name]) \u001b[39mfor\u001b[39;00m name, pi \u001b[39min\u001b[39;00m qedProperties\u001b[39m.\u001b[39m_asdict()\u001b[39m.\u001b[39mitems()]\n\u001b[0;32m    205\u001b[0m t \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(wi \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mlog(di) \u001b[39mfor\u001b[39;00m wi, di \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(w, d))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\rdkit\\Chem\\QED.py:169\u001b[0m, in \u001b[0;36mproperties\u001b[1;34m(mol)\u001b[0m\n\u001b[0;32m    164\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mYou need to provide a mol argument.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    165\u001b[0m mol \u001b[39m=\u001b[39m Chem\u001b[39m.\u001b[39mRemoveHs(mol)\n\u001b[0;32m    166\u001b[0m qedProperties \u001b[39m=\u001b[39m QEDproperties(\n\u001b[0;32m    167\u001b[0m   MW\u001b[39m=\u001b[39mrdmd\u001b[39m.\u001b[39m_CalcMolWt(mol),\n\u001b[0;32m    168\u001b[0m   ALOGP\u001b[39m=\u001b[39mCrippen\u001b[39m.\u001b[39mMolLogP(mol),\n\u001b[1;32m--> 169\u001b[0m   HBA\u001b[39m=\u001b[39m\u001b[39msum\u001b[39m(\n\u001b[0;32m    170\u001b[0m     \u001b[39mlen\u001b[39m(mol\u001b[39m.\u001b[39mGetSubstructMatches(pattern)) \u001b[39mfor\u001b[39;00m pattern \u001b[39min\u001b[39;00m Acceptors\n\u001b[0;32m    171\u001b[0m     \u001b[39mif\u001b[39;00m mol\u001b[39m.\u001b[39mHasSubstructMatch(pattern)),\n\u001b[0;32m    172\u001b[0m   HBD\u001b[39m=\u001b[39mrdmd\u001b[39m.\u001b[39mCalcNumHBD(mol),\n\u001b[0;32m    173\u001b[0m   PSA\u001b[39m=\u001b[39mMolSurf\u001b[39m.\u001b[39mTPSA(mol),\n\u001b[0;32m    174\u001b[0m   ROTB\u001b[39m=\u001b[39mrdmd\u001b[39m.\u001b[39mCalcNumRotatableBonds(mol, rdmd\u001b[39m.\u001b[39mNumRotatableBondsOptions\u001b[39m.\u001b[39mStrict),\n\u001b[0;32m    175\u001b[0m   AROM\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(Chem\u001b[39m.\u001b[39mGetSSSR(Chem\u001b[39m.\u001b[39mDeleteSubstructs(Chem\u001b[39m.\u001b[39mMol(mol), AliphaticRings))),\n\u001b[0;32m    176\u001b[0m   ALERTS\u001b[39m=\u001b[39m\u001b[39msum\u001b[39m(\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m alert \u001b[39min\u001b[39;00m StructuralAlerts \u001b[39mif\u001b[39;00m mol\u001b[39m.\u001b[39mHasSubstructMatch(alert)),\n\u001b[0;32m    177\u001b[0m )\n\u001b[0;32m    178\u001b[0m \u001b[39m# The replacement\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[39m# AROM=Lipinski.NumAromaticRings(mol),\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[39m# is not identical. The expression above tends to count more rings\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[39m# N1C2=CC=CC=C2SC3=C1C=CC4=C3C=CC=C4\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39m# OC1=C(O)C=C2C(=C1)OC3=CC(=O)C(=CC3=C2C4=CC=CC=C4)O\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m# CC(C)C1=CC2=C(C)C=CC2=C(C)C=C1  uses 2, should be 0 ?\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m qedProperties\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\rdkit\\Chem\\QED.py:171\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    164\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mYou need to provide a mol argument.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    165\u001b[0m mol \u001b[39m=\u001b[39m Chem\u001b[39m.\u001b[39mRemoveHs(mol)\n\u001b[0;32m    166\u001b[0m qedProperties \u001b[39m=\u001b[39m QEDproperties(\n\u001b[0;32m    167\u001b[0m   MW\u001b[39m=\u001b[39mrdmd\u001b[39m.\u001b[39m_CalcMolWt(mol),\n\u001b[0;32m    168\u001b[0m   ALOGP\u001b[39m=\u001b[39mCrippen\u001b[39m.\u001b[39mMolLogP(mol),\n\u001b[0;32m    169\u001b[0m   HBA\u001b[39m=\u001b[39m\u001b[39msum\u001b[39m(\n\u001b[0;32m    170\u001b[0m     \u001b[39mlen\u001b[39m(mol\u001b[39m.\u001b[39mGetSubstructMatches(pattern)) \u001b[39mfor\u001b[39;00m pattern \u001b[39min\u001b[39;00m Acceptors\n\u001b[1;32m--> 171\u001b[0m     \u001b[39mif\u001b[39;00m mol\u001b[39m.\u001b[39;49mHasSubstructMatch(pattern)),\n\u001b[0;32m    172\u001b[0m   HBD\u001b[39m=\u001b[39mrdmd\u001b[39m.\u001b[39mCalcNumHBD(mol),\n\u001b[0;32m    173\u001b[0m   PSA\u001b[39m=\u001b[39mMolSurf\u001b[39m.\u001b[39mTPSA(mol),\n\u001b[0;32m    174\u001b[0m   ROTB\u001b[39m=\u001b[39mrdmd\u001b[39m.\u001b[39mCalcNumRotatableBonds(mol, rdmd\u001b[39m.\u001b[39mNumRotatableBondsOptions\u001b[39m.\u001b[39mStrict),\n\u001b[0;32m    175\u001b[0m   AROM\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(Chem\u001b[39m.\u001b[39mGetSSSR(Chem\u001b[39m.\u001b[39mDeleteSubstructs(Chem\u001b[39m.\u001b[39mMol(mol), AliphaticRings))),\n\u001b[0;32m    176\u001b[0m   ALERTS\u001b[39m=\u001b[39m\u001b[39msum\u001b[39m(\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m alert \u001b[39min\u001b[39;00m StructuralAlerts \u001b[39mif\u001b[39;00m mol\u001b[39m.\u001b[39mHasSubstructMatch(alert)),\n\u001b[0;32m    177\u001b[0m )\n\u001b[0;32m    178\u001b[0m \u001b[39m# The replacement\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[39m# AROM=Lipinski.NumAromaticRings(mol),\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[39m# is not identical. The expression above tends to count more rings\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[39m# N1C2=CC=CC=C2SC3=C1C=CC4=C3C=CC=C4\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39m# OC1=C(O)C=C2C(=C1)OC3=CC(=O)C(=CC3=C2C4=CC=CC=C4)O\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m# CC(C)C1=CC2=C(C)C=CC2=C(C)C=C1  uses 2, should be 0 ?\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m qedProperties\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_smiles_datasets=['ames', 'bbb_martins', 'cyp1a2_veith', 'cyp2c19_veith','herg_karim','lipophilicity_astrazeneca']\n",
    "split ='_50'\n",
    "device=torch.device('cpu')\n",
    "structural_statistic=['WL', 'nspdk','degree','cluster', 'spectral']#'common_substructures']\n",
    "metrics_type=['nn','structural']#'molecular']\n",
    "#leave it blank to compute all the structural structures \n",
    "generators=['train2']\n",
    "experiments=[5,10,25,33,40,50,66,75,100]\n",
    "device=torch.device('cpu')\n",
    "metrics={}\n",
    "for generator in generators:\n",
    "    for dataset_name in current_smiles_datasets:\n",
    "            print(dataset_name+split)\n",
    "            graphs, splits =get_data(dataset_name+split, return_smiles=True)\n",
    "            train1_smiles=splits['train1_pos_smiles']+splits['train1_neg_smiles'] \n",
    "            train2_smiles=splits['train2_pos_smiles']+splits['train2_neg_smiles'] \n",
    "            metrics=get_all_metrics(gen=train2_smiles,train=train1_smiles,test=train1_smiles)\n",
    "            metrics['dataset']=dataset_name+split\n",
    "            metrics['generator_name']=generator\n",
    "            print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic_1_25\n",
      "data/SWINGNN_experiments/5-percent/synthetic_1/synthetic_1_25/pos/*.pkl\n",
      "Now computing classifier based metrics\n",
      "WARNING:tensorflow:From C:\\Users\\cosmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow_gan\\python\\eval\\classifier_metrics.py:1157: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "Now computing structural based metrics\n",
      "Cannot compute these structural metrics\n",
      "Now computing the auc_roc based  metric\n",
      "Error when computing AUC_ROC with NSPDK\n",
      "{'activations_time': 0.09308505058288574, 'fid': 1992.090677079746, 'fid_time': 0.09808611869812012, 'kid': 32063.271, 'kid_time': 44.243937969207764, 'precision': 0.01485148514851485, 'recall': 0.025, 'f1_pr': 0.018644188556302177, 'precision_time': 0.16258978843688965, 'recall_time': 0.16258978843688965, 'f1_pr_time': 0.16258978843688965, 'density': 0.0029702970297029703, 'coverage': 0.005, 'f1_dc': 0.0037373549602240336, 'density_time': 0.1354506015777588, 'coverage_time': 0.1354506015777588, 'f1_dc_time': 0.1354506015777588, 'mmd_rbf': 1.2035618349909782, 'mmd_rbf_time': 0.16411471366882324, 'mmd_linear': 1856.7135, 'mmd_linear_time': 0.09308505058288574, 'wl_mmd': 0.00995049504950495, 'wl_mmd_time': 0.5358288288116455, 'nspdk_mmd': 0.02885086425015604, 'nspdk_mmd_time': 7.5875372886657715, 'degree_mmd': 0.020658597728897554, 'degree_mmd_time': 19.649858, 'cluster_mmd': 0.12824140737428769, 'cluster_mmd_time': 23.968874, 'dataset': 'synthetic_1_25', 'generator_name': 'rewired', 'degree_of_perturbation': 5}\n",
      "synthetic_1_25\n",
      "data/SWINGNN_experiments/10-percent/synthetic_1/synthetic_1_25/pos/*.pkl\n",
      "Now computing classifier based metrics\n",
      "Now computing structural based metrics\n"
     ]
    }
   ],
   "source": [
    "structural_statistic=['WL', 'nspdk','degree','cluster', 'spectral']\n",
    "device=torch.device('cpu')\n",
    "metrics_type=['nn','structural', 'auc_roc']\n",
    "generators=['swingnn']\n",
    "splits=['_25','_33','_40','_50']\n",
    "experiments=[5,10,25,33,40,50,66,75,100]\n",
    "metrics={}\n",
    "for generator in ['swingnn']:\n",
    "    for dataset_name in ['synthetic_1','synthetic_2']:\n",
    "        for split in splits:\n",
    "            for percent in experiments:\n",
    "                print(dataset_name+split)\n",
    "                train1_graphs,train1_targets,train2_graphs,train2_targets,test_graphs,test_targets=get_graph_data(dataset_name+split, path=r'data/graphs/datasets')\n",
    "                generated_graphs, generated_targets= get_swingnn_graph_data(dataset_name,percent,split)\n",
    "                splits_for_auc_roc=[train1_graphs , train1_targets,train2_graphs , train2_targets, test_graphs, test_targets, generated_graphs, generated_targets]\n",
    "                metrics=evaluate(train1_graphs, generated_graphs, device,  metrics_type, structural_statistic,*splits_for_auc_roc)\n",
    "                metrics['dataset']=dataset_name+split\n",
    "                metrics['generator_name']=generator\n",
    "                metrics['numer_of_epochs_trained']=(percent*5000)/100\n",
    "                print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic_1_25\n",
      "Now computing classifier based metrics\n",
      "WARNING:tensorflow:From C:\\Users\\cosmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow_gan\\python\\eval\\classifier_metrics.py:1157: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.map_fn(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
      "Now computing structural based metrics\n",
      "Cannot compute these structural metrics\n",
      "Now computing the auc_roc based  metric\n"
     ]
    }
   ],
   "source": [
    "structural_statistic=['WL', 'nspdk','degree','cluster', 'spectral']\n",
    "device=torch.device('cpu')\n",
    "metrics_type=['nn','structural', 'auc_roc']\n",
    "generators=['train2']\n",
    "splits=['_25','_33','_40','_50']\n",
    "experiments=[5,10,25,33,40,50,66,75,100]\n",
    "metrics={}\n",
    "for generator in ['train2']:\n",
    "    for dataset_name in ['synthetic_1','synthetic_2']:\n",
    "        for split in splits:\n",
    "            print(dataset_name+split)\n",
    "            train1_graphs,train1_targets,train2_graphs,train2_targets,test_graphs,test_targets=get_graph_data(dataset_name+split, path=r'data/graphs/datasets')\n",
    "            generated_graphs, generated_targets=train2_graphs,train2_targets\n",
    "            splits_for_auc_roc=[train1_graphs , train1_targets,train2_graphs , train2_targets, test_graphs, test_targets, generated_graphs, generated_targets]\n",
    "            metrics=evaluate(train1_graphs, generated_graphs, device,  metrics_type, structural_statistic,*splits_for_auc_roc)\n",
    "            metrics['dataset']=dataset_name+split\n",
    "            metrics['generator_name']=generator\n",
    "            print(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06a8be8316eed2f6558da5cb68a9abde15f0a0ec61139bf81bc916fa7c6839e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
