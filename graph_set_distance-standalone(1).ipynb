{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e13d8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T10:04:08.630431Z",
     "start_time": "2023-06-30T10:04:05.242459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style><style>.output_png {display: table-cell;text-align: center;vertical-align: middle;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import HTML\n",
    "HTML('<style>.container { width:95% !important; }</style><style>.output_png {display: table-cell;text-align: center;vertical-align: middle;}</style>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3af084f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T10:04:08.708097Z",
     "start_time": "2023-06-30T10:04:08.633607Z"
    }
   },
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c9d478",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T10:04:08.865539Z",
     "start_time": "2023-06-30T10:04:08.713217Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "from toolz import curry\n",
    "\n",
    "\n",
    "def hash_list(seq):\n",
    "    return hash(tuple(seq))\n",
    "\n",
    "\n",
    "def hash_value(value, context=1, nbits=None):\n",
    "    if nbits is None: bitmask = 4294967295\n",
    "    else: bitmask = pow(2, nbits) - 1\n",
    "    code = hash((value, context)) & bitmask\n",
    "    #code += 1 # do this to reserve code 0: REMOVED so to work with 2^n sizes\n",
    "    return code\n",
    "\n",
    "\n",
    "@curry\n",
    "def node_neighborhood_hash(u, graph=None):\n",
    "    uh = hash(graph.nodes[u]['label'])\n",
    "    edges_h = [hash((hash(graph.nodes[v]['label']), hash(graph.edges[u, v]['label']))) for v in graph.neighbors(u)]\n",
    "    nh = hash_list(sorted(edges_h))\n",
    "    ext_node_h = hash((uh, nh))\n",
    "    return ext_node_h\n",
    "\n",
    "\n",
    "def rooted_breadth_first_hash(graph, root):\n",
    "    def invert_dict(mydict):\n",
    "        reversed_dict = defaultdict(list)\n",
    "        for key, value in mydict.items(): reversed_dict[value].append(key)\n",
    "        return reversed_dict\n",
    "\n",
    "    node_neighborhood_hash_func = node_neighborhood_hash(graph=graph)\n",
    "    gid_dist_dict = nx.single_source_shortest_path_length(graph, root)\n",
    "    dist_gids_dict = invert_dict(gid_dist_dict)\n",
    "    distance_based_hashes = [sorted(list(map(node_neighborhood_hash_func, dist_gids_dict[d]))) for d in sorted(dist_gids_dict)]\n",
    "    hash_bfs = [hash_list(seq) for seq in distance_based_hashes]\n",
    "    return hash_list(hash_bfs)\n",
    "\n",
    "\n",
    "def nocontext_nodes_hashes(graph):\n",
    "    nocontext_nodes_hashes_list = [rooted_breadth_first_hash(graph, u) for u in graph.nodes()]\n",
    "    return nocontext_nodes_hashes_list\n",
    "\n",
    "\n",
    "def nocontext_edges_hashes(graph):\n",
    "    nocontext_nodes_hashes_list = nocontext_nodes_hashes(graph)\n",
    "    nocontext_nodes_hashes_dict = {u:nocontext_node_hash for u, nocontext_node_hash in zip(graph.nodes(), nocontext_nodes_hashes_list)} \n",
    "    nocontext_edges_hashes_list = [(*sorted([nocontext_nodes_hashes_dict[u], nocontext_nodes_hashes_dict[v]]), hash(graph.edges[u, v]['label'])) for u,v in graph.edges()]\n",
    "    return nocontext_edges_hashes_list, nocontext_nodes_hashes_list\n",
    "\n",
    "\n",
    "def nodes_hash(orig_graph, context=1, nbits=None, use_node_unlabelled_graph=False, use_edge_unlabelled_graph=False):\n",
    "    if use_node_unlabelled_graph or use_edge_unlabelled_graph: graph = orig_graph.copy()\n",
    "    else: graph = orig_graph\n",
    "    if use_node_unlabelled_graph: \n",
    "        for u in graph.nodes(): graph.nodes[u]['label'] = '-'\n",
    "    if use_edge_unlabelled_graph: \n",
    "        for e in graph.edges(): graph.edges[e]['label'] = '-'\n",
    "    nocontext_edges_hashes_list, nocontext_nodes_hashes_list = nocontext_edges_hashes(graph)\n",
    "    g_hash = hash_list(sorted(nocontext_edges_hashes_list))\n",
    "    nodes_hashes_list = [hash_value((g_hash,nocontext_node_hash), context, nbits) for nocontext_node_hash in nocontext_nodes_hashes_list]\n",
    "    return nodes_hashes_list\n",
    "\n",
    "\n",
    "def graph_hash(orig_graph, context=1, nbits=None, use_node_unlabelled_graph=False, use_edge_unlabelled_graph=False):\n",
    "    if use_node_unlabelled_graph or use_edge_unlabelled_graph: graph = orig_graph.copy()\n",
    "    else: graph = orig_graph\n",
    "    if use_node_unlabelled_graph: \n",
    "        for u in graph.nodes(): graph.nodes[u]['label'] = '-'\n",
    "    if use_edge_unlabelled_graph: \n",
    "        for e in graph.edges(): graph.edges[e]['label'] = '-'\n",
    "    nocontext_edges_hashes_list, nocontext_nodes_hashes_list = nocontext_edges_hashes(graph)\n",
    "    g_hash = hash_list(sorted(nocontext_nodes_hashes_list)+sorted(nocontext_edges_hashes_list))\n",
    "    g_hash = hash_value(g_hash, context, nbits)\n",
    "    return g_hash\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ee05a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T10:04:08.957247Z",
     "start_time": "2023-06-30T10:04:08.868854Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from toolz import curry\n",
    "from collections import defaultdict\n",
    "\n",
    "def neighborhood_decomposition(graph, cutoff=1):\n",
    "    node_bunches = []\n",
    "    for u in graph.nodes():\n",
    "        ego_graph = nx.ego_graph(graph, u, radius=cutoff)\n",
    "        node_bunches.append(list(ego_graph.nodes()))\n",
    "    return node_bunches\n",
    "\n",
    "\n",
    "\n",
    "@curry\n",
    "def neighborhood(graphofgraph, size=1, min_size=None, max_size=None):\n",
    "    signature = function_signature(locals())\n",
    "    if min_size is None:\n",
    "        min_size = size \n",
    "    if max_size is None:\n",
    "        max_size = size\n",
    "    node_bunches = []\n",
    "    node_signatures = []\n",
    "    for s in range(min_size, max_size+1):\n",
    "        for u in graphofgraph.nodes():\n",
    "            subgraph = graphofgraph.nodes[u]['subgraph']\n",
    "            components = neighborhood_decomposition(subgraph, cutoff=s)\n",
    "            node_bunches.extend(components)\n",
    "            node_signature = make_signature(underlying_signature=graphofgraph.nodes[u]['signature'], added_signature=signature)\n",
    "            node_signatures.extend([node_signature]*len(components))\n",
    "    out_graphofgraph = make_graph_of_graph(base_graph=graphofgraph.graph['base'], node_bunches=node_bunches, node_signatures=node_signatures)\n",
    "    return out_graphofgraph\n",
    "\n",
    "\n",
    "def invert_dict(mydict):\n",
    "    reversed_dict = defaultdict(list)\n",
    "    for key, value in mydict.items():\n",
    "        reversed_dict[value].append(key)\n",
    "    return reversed_dict\n",
    "\n",
    "\n",
    "def get_distances(graph, cutoff=None):\n",
    "    return {node_id:invert_dict(nx.single_source_shortest_path_length(graph, node_id, cutoff=cutoff)) for node_id in graph.nodes()}\n",
    "\n",
    "\n",
    "def get_neighborhood(node_id, radius, distances_dict):\n",
    "    nbunch = []\n",
    "    for dist in range(radius+1):\n",
    "        nbunch.extend(distances_dict[node_id][dist])\n",
    "    return nbunch\n",
    "\n",
    "@curry\n",
    "def pairwise_neighborhood(graphofgraph, size=1, min_size=None, max_size=None, distance=0, min_distance=None, max_distance=None):\n",
    "    signature = function_signature(locals())\n",
    "    if min_size is None:\n",
    "        min_size = size \n",
    "    if max_size is None:\n",
    "        max_size = size\n",
    "    if min_distance is None:\n",
    "        min_distance = distance \n",
    "    if max_distance is None:\n",
    "        max_distance = distance\n",
    "\n",
    "    cutoff = max(max_distance, max_size)\n",
    "    node_bunches = []\n",
    "    node_signatures = []\n",
    "    for u in graphofgraph.nodes():\n",
    "        subgraph = graphofgraph.nodes[u]['subgraph']\n",
    "        distances_dict = get_distances(subgraph, cutoff)\n",
    "        for radius in range(min_size, max_size+1):  \n",
    "            for i in subgraph.nodes():\n",
    "                neighborhood_i = get_neighborhood(i, radius, distances_dict)\n",
    "                for dist in range(min_distance, max_distance+1):\n",
    "                    js = distances_dict[i][dist]\n",
    "                    for j in js:\n",
    "                        neighborhood_j = get_neighborhood(j, radius, distances_dict)\n",
    "                        component = set(neighborhood_i+neighborhood_j)\n",
    "                        node_bunches.append(component)\n",
    "                        node_signature = make_signature(underlying_signature=graphofgraph.nodes[u]['signature'], added_signature=signature)\n",
    "                        node_signatures.append(node_signature)\n",
    "    out_graphofgraph = make_graph_of_graph(base_graph=graphofgraph.graph['base'], node_bunches=node_bunches, node_signatures=node_signatures)\n",
    "    return out_graphofgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e92263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T10:04:09.054297Z",
     "start_time": "2023-06-30T10:04:08.960908Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from toolz import curry\n",
    "\n",
    "\n",
    "def get_edges_from_cycle(cycle):\n",
    "    for i, c in enumerate(cycle):\n",
    "        j = (i + 1) % len(cycle)\n",
    "        u, v = cycle[i], cycle[j]\n",
    "        if u < v:\n",
    "            yield u, v\n",
    "        else:\n",
    "            yield v, u\n",
    "\n",
    "\n",
    "def get_cycle_basis_edges(g):\n",
    "    ebunch = []\n",
    "    cs = nx.cycle_basis(g)\n",
    "    for c in cs:\n",
    "        ebunch += list(get_edges_from_cycle(c))\n",
    "    return ebunch\n",
    "\n",
    "\n",
    "def edge_complement(g, ebunch):\n",
    "    edge_set = set(ebunch)\n",
    "    other_ebunch = [e for e in g.edges() if e not in edge_set]\n",
    "    return other_ebunch\n",
    "\n",
    "\n",
    "def edge_subgraph(g, ebunch):\n",
    "    if nx.is_directed(g):\n",
    "        g2 = nx.DiGraph()\n",
    "    else:\n",
    "        g2 = nx.Graph()\n",
    "    g2.add_nodes_from(g.nodes())\n",
    "    for u, v in ebunch:\n",
    "        g2.add_edge(u, v)\n",
    "        g2.edges[u, v].update(g.edges[u, v])\n",
    "    return g2\n",
    "\n",
    "\n",
    "def edge_complement_subgraph(g, ebunch):\n",
    "    \"\"\"Induce graph from edges that are not in ebunch.\"\"\"\n",
    "    if nx.is_directed(g):\n",
    "        g2 = nx.DiGraph()\n",
    "    else:\n",
    "        g2 = nx.Graph()\n",
    "    g2.add_nodes_from(g.nodes())\n",
    "    for e in g.edges():\n",
    "        if e not in ebunch:\n",
    "            u, v = e\n",
    "            g2.add_edge(u, v)\n",
    "            g2.edges[u, v].update(g.edges[u, v])\n",
    "    return g2\n",
    "\n",
    "\n",
    "def cycle_basis_and_non_cycle_decomposition(g, min_size=None, max_size=None):\n",
    "    cs = nx.cycle_basis(g)\n",
    "    cycle_components = list(map(set, cs))\n",
    "    if min_size is not None and max_size is not None:\n",
    "        cycle_components = [cyc for cyc in cycle_components if len(cyc)>= min_size and len(cyc)<= max_size ]\n",
    "    cycle_ebunch = get_cycle_basis_edges(g)\n",
    "    g2 = edge_complement_subgraph(g, cycle_ebunch)\n",
    "    non_cycle_components = nx.connected_components(g2)\n",
    "    non_cycle_components = [c for c in non_cycle_components if len(c) >= 2]\n",
    "    non_cycle_components = list(map(set, non_cycle_components))\n",
    "    if min_size is not None and max_size is not None:\n",
    "        non_cycle_components = [cyc for cyc in non_cycle_components if len(cyc)>= min_size and len(cyc)<= max_size ]\n",
    "    return cycle_components, non_cycle_components\n",
    "\n",
    "\n",
    "@curry\n",
    "def cycle(graphofgraph, size=None, min_size=None, max_size=None, use_positive=True, use_negative=True, abstraction_level='graph_process'):\n",
    "    signature = function_signature(locals())\n",
    "    if min_size is None:\n",
    "        min_size = size \n",
    "    if max_size is None:\n",
    "        max_size = size\n",
    "    node_bunches = []\n",
    "    node_signatures = []\n",
    "    for u in graphofgraph.nodes():\n",
    "        subgraph = graphofgraph.nodes[u]['subgraph']\n",
    "        positive_components, negative_components = cycle_basis_and_non_cycle_decomposition(subgraph, min_size=min_size, max_size=max_size)\n",
    "        if use_positive:\n",
    "            node_bunches.extend(positive_components)\n",
    "            node_signature = make_signature(underlying_signature=graphofgraph.nodes[u]['signature'], added_signature=signature, use_positive=True, use_negative=None)\n",
    "            node_signatures.extend([node_signature]*len(positive_components))\n",
    "        if use_negative: \n",
    "            node_bunches.extend(negative_components)\n",
    "            node_signature = make_signature(underlying_signature=graphofgraph.nodes[u]['signature'], added_signature=signature, use_positive=None, use_negative=True)\n",
    "            node_signatures.extend([node_signature]*len(negative_components))\n",
    "    out_graphofgraph = make_graph_of_graph(base_graph=graphofgraph.graph['base'], node_bunches=node_bunches, node_signatures=node_signatures, abstraction_level=abstraction_level)\n",
    "    return out_graphofgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4993180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T10:04:09.134946Z",
     "start_time": "2023-06-30T10:04:09.057495Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from toolz import curry\n",
    "\n",
    "\n",
    "@curry\n",
    "def atom(graphofgraph, use_nodes=True, use_edges=True):\n",
    "    signature = function_signature(locals())\n",
    "    node_bunches = []\n",
    "    node_signatures = []\n",
    "    for u in graphofgraph.nodes():\n",
    "        subgraph = graphofgraph.nodes[u]['subgraph']\n",
    "        if use_nodes is True:\n",
    "            for n in subgraph.nodes():\n",
    "                node_bunches.append([n])\n",
    "                node_signature = make_signature(underlying_signature=graphofgraph.nodes[u]['signature'], added_signature=signature+'+n')\n",
    "                node_signatures.append(node_signature)\n",
    "        if use_edges is True:\n",
    "            for i,j in subgraph.edges():\n",
    "                node_bunches.append([i,j])\n",
    "                node_signature = make_signature(underlying_signature=graphofgraph.nodes[u]['signature'], added_signature=signature+'+e')\n",
    "                node_signatures.append(node_signature)\n",
    "    out_graphofgraph = make_graph_of_graph(base_graph=graphofgraph.graph['base'], node_bunches=node_bunches, node_signatures=node_signatures)\n",
    "    return out_graphofgraph\n",
    "\n",
    "\n",
    "@curry\n",
    "def node(graphofgraph, flag=None):\n",
    "    return atom(graphofgraph, use_nodes=True, use_edges=False)\n",
    "\n",
    "@curry\n",
    "def edge(graphofgraph, flag=None):\n",
    "    return atom(graphofgraph, use_nodes=False, use_edges=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d880b96a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T10:04:09.382646Z",
     "start_time": "2023-06-30T10:04:09.138506Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from toolz import partition_all\n",
    "import multiprocessing_on_dill as mp\n",
    "import inspect\n",
    "\n",
    "def function_signature(function_arguments_dict):\n",
    "    if 'graphofgraph' in function_arguments_dict: function_arguments_dict.pop('graphofgraph','')\n",
    "    function_name = inspect.stack()[1][3]\n",
    "    signature = function_name + str(function_arguments_dict)\n",
    "    return signature\n",
    "\n",
    "def serial_decomposition(graphs, decomposition_function, nbits):\n",
    "    graphofgraphs = [decomposition_function(construct(graph, nbits=nbits)) for graph in graphs]\n",
    "    return graphofgraphs\n",
    "\n",
    "def parallel_decomposition(graphs, decomposition_function, nbits):\n",
    "    def _make_decomposition_func(decomposition_function, nbits):\n",
    "        def decomposition_func(graphs):\n",
    "            return serial_decomposition(graphs, decomposition_function, nbits=nbits)\n",
    "        return decomposition_func\n",
    "\n",
    "    n_cpus = mp.cpu_count()\n",
    "    batch_size = len(graphs)//n_cpus\n",
    "    if batch_size < 2:\n",
    "        graphs_list = [graphs]\n",
    "    else:    \n",
    "        graphs_list = list(partition_all(batch_size, graphs))\n",
    "    decomposition_func = _make_decomposition_func(decomposition_function, nbits)\n",
    "    pool = mp.Pool(n_cpus)\n",
    "    results = pool.map(decomposition_func, graphs_list)\n",
    "    pool.close()\n",
    "    all_list_of_mtx = []\n",
    "    for list_of_mtx in results:\n",
    "        all_list_of_mtx.extend(list_of_mtx)\n",
    "    return all_list_of_mtx\n",
    "\n",
    "def decomposition(graphs, decomposition_function, nbits, parallel=True):\n",
    "    if parallel == True:\n",
    "        graphofgraphs = parallel_decomposition(graphs, decomposition_function, nbits)\n",
    "    else:\n",
    "        graphofgraphs = serial_decomposition(graphs, decomposition_function, nbits)\n",
    "    return graphofgraphs\n",
    "\n",
    "\n",
    "def make_signature(underlying_signature='', added_signature='', min_size=None, max_size=None, use_positive=None, use_negative=None):\n",
    "    sfx = ''\n",
    "    if use_positive is True:\n",
    "        sfx += '+'\n",
    "    if use_negative is True:\n",
    "        sfx += '-'\n",
    "    signature = '%s%s'%(added_signature,sfx)\n",
    "    if min_size is not None and max_size is not None:\n",
    "        signature += '%d:%d'%(min_size,max_size)\n",
    "    if underlying_signature != 'base':\n",
    "        signature += '(%s)'%(underlying_signature)\n",
    "    return signature\n",
    "\n",
    "def make_edges_of_graph_of_graph(graphofgraph):\n",
    "    nbits = graphofgraph.graph['base'].graph['nbits']\n",
    "    for edge_id in graphofgraph.edges():\n",
    "        u,v = edge_id\n",
    "        edge_signature = graphofgraph.edges[edge_id]['signature']\n",
    "        signature_hash = hash(edge_signature)\n",
    "        edge_label = hash(tuple(sorted([graphofgraph.nodes[u]['label'],graphofgraph.nodes[v]['label']])))\n",
    "        edge_label = hash_value(edge_label, context=signature_hash, nbits=nbits)\n",
    "        graphofgraph.edges[edge_id]['label'] = edge_label\n",
    "        process_hash = hash_value(edge_signature, context=signature_hash, nbits=nbits)\n",
    "        graphofgraph.edges[edge_id]['process_hash'] = process_hash\n",
    "    return graphofgraph\n",
    "\n",
    "def make_subgraphs_graph_of_graph(base_graph, bunches=None, signatures=[], subgraph_mode='node', abstraction_level='graph_process'):\n",
    "    graphofgraph = nx.Graph()\n",
    "    nbits = base_graph.graph['nbits']\n",
    "    for u in base_graph.nodes(): base_graph.nodes[u]['location_graph_process_hash'] = []\n",
    "    for u in base_graph.nodes(): base_graph.nodes[u]['location_node_unlabelled_graph_process_hash'] = []\n",
    "    for u in base_graph.nodes(): base_graph.nodes[u]['location_edge_unlabelled_graph_process_hash'] = []\n",
    "    for u in base_graph.nodes(): base_graph.nodes[u]['location_unlabelled_graph_process_hash'] = []\n",
    "    for u in base_graph.nodes(): base_graph.nodes[u]['graph_process_hash'] = []\n",
    "    for u in base_graph.nodes(): base_graph.nodes[u]['node_unlabelled_graph_process_hash'] = []\n",
    "    for u in base_graph.nodes(): base_graph.nodes[u]['edge_unlabelled_graph_process_hash'] = []\n",
    "    for u in base_graph.nodes(): base_graph.nodes[u]['unlabelled_graph_process_hash'] = []\n",
    "    for u in base_graph.nodes(): base_graph.nodes[u]['process_hash'] = []\n",
    "    \n",
    "    #add graph_process_hash and process_hash to graphofgraph nodes\n",
    "    for u, (bunch, signature) in enumerate(zip(bunches, signatures)):\n",
    "        if subgraph_mode == 'node':\n",
    "            subgraph = nx.subgraph(base_graph, bunch)\n",
    "        elif subgraph_mode == 'edge':\n",
    "            subgraph = nx.edge_subgraph(base_graph, bunch)\n",
    "        signature_hash = hash(signature)\n",
    "        process_hash = hash_value(signature, context=signature_hash, nbits=nbits)\n",
    "        graph_process_hash = graph_hash(subgraph, context=signature_hash, nbits=nbits, use_node_unlabelled_graph=False, use_edge_unlabelled_graph=False)\n",
    "        node_unlabelled_graph_process_hash = graph_hash(subgraph, context=signature_hash, nbits=nbits, use_node_unlabelled_graph=True, use_edge_unlabelled_graph=False)     \n",
    "        edge_unlabelled_graph_process_hash = graph_hash(subgraph, context=signature_hash, nbits=nbits, use_node_unlabelled_graph=False, use_edge_unlabelled_graph=True)\n",
    "        unlabelled_graph_process_hash = graph_hash(subgraph, context=signature_hash, nbits=nbits, use_node_unlabelled_graph=True, use_edge_unlabelled_graph=True)\n",
    "        if abstraction_level=='graph_process': label = graph_process_hash\n",
    "        if abstraction_level=='node_unlabelled_graph_process': label = node_unlabelled_graph_process_hash\n",
    "        if abstraction_level=='edge_unlabelled_graph_process': label = edge_unlabelled_graph_process_hash\n",
    "        if abstraction_level=='unlabelled_graph_process': label = unlabelled_graph_process_hash\n",
    "        if abstraction_level=='process': label = process_hash\n",
    "        graphofgraph.add_node(\n",
    "            u, \n",
    "            label=label, \n",
    "            graph_process_hash=graph_process_hash, \n",
    "            node_unlabelled_graph_process_hash=node_unlabelled_graph_process_hash,\n",
    "            edge_unlabelled_graph_process_hash=edge_unlabelled_graph_process_hash,\n",
    "            unlabelled_graph_process_hash=unlabelled_graph_process_hash,\n",
    "            process_hash=process_hash, \n",
    "            subgraph=nx.Graph(subgraph), \n",
    "            signature=signature)\n",
    "    \n",
    "    #add location_graph_process_hash to base_graph nodes\n",
    "    #append graph_process_hash and process_hash for each graphofgraph nodes they are in to base_graph nodes\n",
    "    for u in graphofgraph.nodes():\n",
    "        subgraph = graphofgraph.nodes[u]['subgraph']\n",
    "        graph_process_hash = graphofgraph.nodes[u]['graph_process_hash']\n",
    "        node_unlabelled_graph_process_hash = graphofgraph.nodes[u]['node_unlabelled_graph_process_hash']\n",
    "        edge_unlabelled_graph_process_hash = graphofgraph.nodes[u]['edge_unlabelled_graph_process_hash']\n",
    "        unlabelled_graph_process_hash = graphofgraph.nodes[u]['unlabelled_graph_process_hash']\n",
    "        process_hash = graphofgraph.nodes[u]['process_hash']\n",
    "        location_graph_process_hash_list = nodes_hash(subgraph, context=graph_process_hash, nbits=nbits, use_node_unlabelled_graph=False, use_edge_unlabelled_graph=False)\n",
    "        location_node_unlabelled_graph_process_hash_list = nodes_hash(subgraph, context=node_unlabelled_graph_process_hash, nbits=nbits, use_node_unlabelled_graph=True, use_edge_unlabelled_graph=False)\n",
    "        location_edge_unlabelled_graph_process_hash_list = nodes_hash(subgraph, context=edge_unlabelled_graph_process_hash, nbits=nbits, use_node_unlabelled_graph=False, use_edge_unlabelled_graph=True)\n",
    "        location_unlabelled_graph_process_hash_list = nodes_hash(subgraph, context=unlabelled_graph_process_hash, nbits=nbits, use_node_unlabelled_graph=True, use_edge_unlabelled_graph=True)\n",
    "        for node_id, location_graph_process_hash, location_node_unlabelled_graph_process_hash, location_edge_unlabelled_graph_process_hash, location_unlabelled_graph_process_hash in zip(subgraph.nodes(),location_graph_process_hash_list, location_node_unlabelled_graph_process_hash_list, location_edge_unlabelled_graph_process_hash_list, location_unlabelled_graph_process_hash_list):\n",
    "            base_graph.nodes[node_id]['location_graph_process_hash'].append(location_graph_process_hash)\n",
    "            base_graph.nodes[node_id]['location_node_unlabelled_graph_process_hash'].append(location_node_unlabelled_graph_process_hash)\n",
    "            base_graph.nodes[node_id]['location_edge_unlabelled_graph_process_hash'].append(location_edge_unlabelled_graph_process_hash)\n",
    "            base_graph.nodes[node_id]['location_unlabelled_graph_process_hash'].append(location_unlabelled_graph_process_hash)\n",
    "        for node_id in graphofgraph.nodes[u]['subgraph'].nodes():\n",
    "            base_graph.nodes[node_id]['graph_process_hash'].append(graph_process_hash)\n",
    "            base_graph.nodes[node_id]['node_unlabelled_graph_process_hash'].append(node_unlabelled_graph_process_hash)\n",
    "            base_graph.nodes[node_id]['edge_unlabelled_graph_process_hash'].append(edge_unlabelled_graph_process_hash)\n",
    "            base_graph.nodes[node_id]['unlabelled_graph_process_hash'].append(unlabelled_graph_process_hash)\n",
    "            base_graph.nodes[node_id]['process_hash'].append(process_hash)\n",
    "    graphofgraph.graph['base'] = base_graph\n",
    "    return graphofgraph\n",
    "\n",
    "\n",
    "def make_graph_of_graph(base_graph, node_bunches=None, edge_bunches=None, edges=None, node_signatures=[], edge_signatures=[], abstraction_level='graph_process'):\n",
    "    \"\"\"\n",
    "    Make a graph of graph starting from the base graph in 'base_graph' and a list of lists of node ids or list of lists of edges ids. \n",
    "\n",
    "    node_bunches: Each list of nodes is used to induce a subgraph to be associated to a node of the graph of graph.\n",
    "    edge_bunches: Each subgraph can be identified by a list of nodes, or a list of edges (in this case it is a edge induced subgraph).\n",
    "    edges: Edges can be provided explicitly between nodes of the graph of graph. \n",
    "    The label of a node of the graph of graph is computed as a specific permutation invariant hash of the subgraph.\n",
    "    A 'signature' string is used to seed the hash function so that two isomophic subgraphs that are produced by different procedures get a distinct encoding. \n",
    "    \"\"\"\n",
    "    if node_bunches is not None:\n",
    "        return make_subgraphs_graph_of_graph(base_graph, bunches=node_bunches, signatures=node_signatures, subgraph_mode='node', abstraction_level=abstraction_level)\n",
    "    if edge_bunches is not None:\n",
    "        return make_subgraphs_graph_of_graph(base_graph, bunches=edge_bunches, signatures=edge_signatures, subgraph_mode='edge', abstraction_level=abstraction_level)\n",
    "    return graphofgraph\n",
    "\n",
    "\n",
    "def get_node_bunches(graphofgraph):\n",
    "    return [list(graphofgraph.nodes[u]['subgraph'].nodes()) for u in graphofgraph.nodes()]\n",
    "\n",
    "\n",
    "def get_node_subgraphs(graphofgraph):\n",
    "    return [graphofgraph.nodes[u]['subgraph'] for u in graphofgraph.nodes()]\n",
    "\n",
    "\n",
    "def get_edge_subgraphs(graphofgraph):\n",
    "    return [(graphofgraph.nodes[u]['subgraph'],graphofgraph.nodes[v]['subgraph']) for u,v in graphofgraph.edges()]\n",
    "\n",
    "\n",
    "def construct(graph, attribute_label='vec', nbits=16):\n",
    "    \"\"\"\n",
    "    Construct a graph of graph from a base graph.\n",
    "\n",
    "    A graph of graph is a graph that has as nodes subgraphs of a base graph and as edges relations between these subgraphs.\n",
    "    The default constructor builds a graph of graph made of a single node which has a subgraph the whole base graph.\n",
    "    The attribute_label is the dictionary key that allow access to real valued arrays for each node in the base graph.\n",
    "    \"\"\"\n",
    "    base_graph = nx.Graph(graph)\n",
    "    base_graph = nx.convert_node_labels_to_integers(base_graph)\n",
    "    base_graph.graph['nbits'] = nbits\n",
    "    base_graph.graph['bitmask'] = pow(2, nbits) - 1\n",
    "    base_graph.graph['feature_size'] = base_graph.graph['bitmask'] + 1\n",
    "    base_graph.graph['attribute_label'] = attribute_label\n",
    "    node_bunches = [list(base_graph.nodes())]\n",
    "    graphofgraph = make_graph_of_graph(base_graph, node_bunches=node_bunches, node_signatures=['base'])\n",
    "    return graphofgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e84873fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T10:04:09.488575Z",
     "start_time": "2023-06-30T10:04:09.385766Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import networkx as nx\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "class GraphSetDistanceEstimator(object):\n",
    "    def __init__(self, decomposition_function, nbits=19, metric='cosine', num_iter=5, parallel=True):\n",
    "        self.decomposition_function = decomposition_function\n",
    "        self.nbits = nbits\n",
    "        self.metric = metric\n",
    "        self.num_iter = num_iter\n",
    "        self.parallel = parallel\n",
    "        \n",
    "    def graph_set_feature_histogram(self, graphofgraphs):\n",
    "        L = [graphofgraph.nodes[u]['label'] for graphofgraph in graphofgraphs for u in graphofgraph.nodes()]    \n",
    "        feature_ids, counts = np.unique(L,return_counts=True)\n",
    "        data = counts\n",
    "        col = feature_ids\n",
    "        row = np.zeros(feature_ids.shape)\n",
    "        hist = csr_matrix((data, (row, col)), shape=(1,2**self.nbits))\n",
    "        return hist\n",
    "\n",
    "    def graph_set_distance(self, source_graphofgraphs, destination_graphofgraphs, metric='cosine'):\n",
    "        source_feature_histogram = self.graph_set_feature_histogram(source_graphofgraphs)\n",
    "        source_feature_histogram[0,0] = 0 #remove feature that counts the number of nodes\n",
    "        source_feature_distribution = source_feature_histogram/source_feature_histogram.sum()\n",
    "        \n",
    "        destination_feature_histogram = self.graph_set_feature_histogram(destination_graphofgraphs)\n",
    "        destination_feature_histogram[0,0] = 0 #remove feature that counts the number of nodes\n",
    "        destination_feature_distribution = destination_feature_histogram/destination_feature_histogram.sum()\n",
    "        \n",
    "        distance = pairwise_distances(X=source_feature_distribution, Y=destination_feature_distribution, metric=metric).flatten()[0]\n",
    "        return distance\n",
    "\n",
    "    def fit(self, graphs):\n",
    "        self.graphofgraphs = decomposition(graphs, decomposition_function=self.decomposition_function, nbits=self.nbits, parallel=self.parallel)\n",
    "        distances = [self.random_half_split_self_distance(self.graphofgraphs) for it in range(self.num_iter)]\n",
    "        self.distance_mean = np.mean(distances)\n",
    "        self.distance_std = np.std(distances)\n",
    "        return self\n",
    "    \n",
    "    def random_half_split_self_distance(self, graphofgraphs):\n",
    "        lim = len(graphofgraphs)//2\n",
    "        random.shuffle(graphofgraphs)\n",
    "        distance = self.graph_set_distance(graphofgraphs[:lim], graphofgraphs[lim:], metric=self.metric)\n",
    "        return distance\n",
    "    \n",
    "    def estimate(self, graphs):\n",
    "        graphofgraphs = decomposition(graphs, decomposition_function=self.decomposition_function, nbits=self.nbits, parallel=self.parallel)\n",
    "        distance = self.graph_set_distance(graphofgraphs, self.graphofgraphs, metric=self.metric)\n",
    "        distance_score = np.absolute(distance-self.distance_mean)/self.distance_std\n",
    "        return distance_score\n",
    "\n",
    "def symmetric_graph_set_distance(graphs_1, graphs_2, decomposition_function, nbits=19, metric='cosine', num_iter=5, parallel=True):\n",
    "    distance_12 = GraphSetDistanceEstimator(decomposition_function=decomposition_function, nbits=nbits, metric=metric, num_iter=num_iter, parallel=parallel).fit(graphs_1).estimate(graphs_2)\n",
    "    distance_21 = GraphSetDistanceEstimator(decomposition_function=decomposition_function, nbits=nbits, metric=metric, num_iter=num_iter, parallel=parallel).fit(graphs_2).estimate(graphs_1)\n",
    "    return np.mean([distance_12, distance_21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cec884b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.mol_structure import list_of_smiles_to_nx_graphs\n",
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "def load_csv_data_from_a_PubChem_assay(assay_id):\n",
    "    #url = f'https://pubchem.ncbi.nlm.nih.gov/assay/pcget.cgi?query=download&record_type=datatable&actvty=all&response_type=save&aid={assay_id}'\n",
    "    url=f'https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{assay_id}/concise/csv'\n",
    "    #df_raw=pd.read_csv(url)\n",
    "    df_raw=pd.read_csv(url)\n",
    "    #print(df_raw.head())\n",
    "    return(df_raw)\n",
    "\n",
    "def drop_sids_with_no_cids(df):\n",
    "    df = df.dropna( subset=['cid'] )\n",
    "    #Remove CIDs with conflicting activities\n",
    "    cid_conflict = []\n",
    "    idx_conflict = []\n",
    "\n",
    "    for mycid in df['cid'].unique() :\n",
    "        \n",
    "        outcomes = df[ df.cid == mycid ].activity.unique()\n",
    "        \n",
    "        if len(outcomes) > 1 :\n",
    "            \n",
    "            idx_tmp = df.index[ df.cid == mycid ].tolist()\n",
    "            idx_conflict.extend(idx_tmp)\n",
    "            cid_conflict.append(mycid)\n",
    "\n",
    "    #print(\"#\", len(cid_conflict), \"CIDs with conflicting activities [associated with\", len(idx_conflict), \"rows (SIDs).]\")\n",
    "    df = df.drop(idx_conflict)\n",
    "\n",
    "    #Remove redundant data\n",
    "\n",
    "    df = df.drop_duplicates(subset='cid')  # remove duplicate rows except for the first occurring row.\n",
    "    #print(len(df['sid'].unique()))\n",
    "    #print(len(df['cid'].unique()))\n",
    "    return df\n",
    "    \n",
    "     \n",
    "\n",
    "def download_smiles_given_cids_from_pubmed(list_of_cids,chunk_size = 200): #returns df of smiles and cids\n",
    "    df_smiles = pd.DataFrame()\n",
    "\n",
    "    num_cids = len(list_of_cids)\n",
    "    list_dfs = []\n",
    "    if num_cids % chunk_size == 0 :\n",
    "        num_chunks = int( num_cids / chunk_size )\n",
    "    else :\n",
    "        num_chunks = int( num_cids / chunk_size ) + 1\n",
    "\n",
    "    #print(\"# CIDs = \", num_cids)\n",
    "    #print(\"# CID Chunks = \", num_chunks, \"(chunked by \", chunk_size, \")\")\n",
    "\n",
    "    for i in range(0, num_chunks) :\n",
    "        idx1 = chunk_size * i\n",
    "        idx2 = chunk_size * (i + 1)\n",
    "        cidstr = \",\".join( str(x) for x in list_of_cids[idx1:idx2] )\n",
    "\n",
    "        url = ('https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/' + cidstr + '/property/IsomericSMILES/TXT')\n",
    "        res = requests.request('GET',url)\n",
    "        data = pd.read_csv( StringIO(res.text), header=None, names=['smiles'] )\n",
    "        list_dfs.append(data)\n",
    "        \n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        #if ( i % 5 == 0 ) :\n",
    "            #print(\"Processing Chunk \", i)\n",
    "    df_smiles = pd.concat(list_dfs,ignore_index=True)\n",
    "    df_smiles[ 'cid' ] = list_of_cids   \n",
    "\n",
    "    return df_smiles\n",
    "\n",
    "def load_PUBCHEM_dataset(assay_id,**kwarg):\n",
    "    df_raw=load_csv_data_from_a_PubChem_assay(assay_id=assay_id)\n",
    "    print(len(df_raw))\n",
    "    #Drop substances without Inconclusive activity\n",
    "    df_raw=df_raw[df_raw['Activity Outcome']!='Inconclusive']\n",
    "    #Select active/inactive compounds for model building\n",
    "    df=df_raw[ (df_raw['Activity Outcome'] == 'Active' ) | \n",
    "             (df_raw['Activity Outcome'] == 'Inactive' ) ].rename(columns={\"CID\": \"cid\", \"SID\":\"sid\",\"Activity Outcome\": \"activity\"})\n",
    "    #drop duplicates, and comnflicting activities, and substances with no cids\n",
    "    df=drop_sids_with_no_cids(df)\n",
    "    #label encoding\n",
    "    df['activity'] = [ 0 if x == 'Inactive' else 1 for x in df['activity'] ]\n",
    "    df_smiles=download_smiles_given_cids_from_pubmed(df.cid.astype(int).tolist())\n",
    "    X=df_smiles.smiles.tolist()\n",
    "    y=df.activity.astype(int).tolist()\n",
    "    return(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d7dbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "def rdkmol_to_nx(mol):\n",
    "    #  rdkit-mol object to nx.graph\n",
    "    graph = nx.Graph()\n",
    "    for atom in mol.GetAtoms():\n",
    "        graph.add_node(atom.GetIdx(), label='1')\n",
    "    for bond in mol.GetBonds():\n",
    "        graph.add_edge(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx(),label='1')\n",
    "    return graph\n",
    "\n",
    "\n",
    "\n",
    "def list_of_smiles_to_nx_graphs(smiles):\n",
    "    list_of_nx_graphs=[]\n",
    "    for i,smile in enumerate(smiles):\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol:\n",
    "           list_of_nx_graphs.append(rdkmol_to_nx(mol))\n",
    "        else:\n",
    "           # print('Check smile entry no', i+1)\n",
    "            list_of_nx_graphs.append(rdkmol_to_nx(Chem.MolFromSmiles('C')))\n",
    "         \n",
    "    return list_of_nx_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa6ec86c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-30T10:04:05.220Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "624249\n",
      "670\n",
      "unlabelled_graph_cycle_distance <function cycle at 0x000001B2EF8175B0>\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import time\n",
    "\n",
    "assay_ids = ['624249','463213','492992','463230','651741','743219','588350','492952','2631','651610','624466']\n",
    "\n",
    "name_distance_list = []\n",
    "name_distance_list.append(['node_edge_distance',atom()])\n",
    "name_distance_list.append(['unlabelled_graph_cycle_distance',cycle(abstraction_level='unlabelled_graph_process')])\n",
    "name_distance_list.append(['cycle_distance', cycle()])\n",
    "name_distance_list.append(['neighborhood_distance', neighborhood(size=1)])\n",
    "name_distance_list.append(['neighborhood_distance_r=2', neighborhood(size=2)])\n",
    "    \n",
    "for assay_id in assay_ids:\n",
    "    print('-'*100)\n",
    "    print(assay_id)\n",
    "    smiles, targets = load_PUBCHEM_dataset(assay_id, dirname='PUBCHEM', format_type='sdf', balance=True, shuffle=True, verbose=True)\n",
    "    graphs,targets=list_of_smiles_to_nx_graphs(smiles), targets\n",
    "    pos_graphs = [graphs[idx] for idx in range(len(graphs)) if targets[idx]==1][:5]\n",
    "    neg_graphs = [graphs[idx] for idx in range(len(graphs)) if targets[idx]!=1][:5]\n",
    "    start = time.time()\n",
    "    #print('wtf')\n",
    "    name, df = name_distance_list[1]\n",
    "    print(name,df)\n",
    "    dist = symmetric_graph_set_distance(pos_graphs, neg_graphs, decomposition_function=df)\n",
    "    print('%35s: %5.2f'%(name, dist))\n",
    "    print('Time elapsed: %.1f min'%((time.time()-start)/60))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15ddf44e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
